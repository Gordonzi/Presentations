h1. Lecture 3: Scaling Systems, Part I

h1. Some Terms &#8224;

<div class='medium'>
* *Client:* A person or other _service_ that uses a provided _service_.
* *Process:* An operating system process.
* *Component:* A "course grained" HW or SW entity. SW _components_ are in the same _process_.
* *Service:* One or more _processes_ and _components_ that provide a subset of _system services_.
* *Network Node:* A discrete server that provides _services_ to other _network nodes_.
* *System:* The well defined collection of _components_, _services_, and _network nodes_.
* *System Point:* Some logically-discrete part of an overall _system_. Depending on the context, it might be a _component_, _service_, or _network node_.

&#8224; Warning, some of these terms are vague; they don't have standard definitions!
</div>

h1. Some Ways that Systems Fail

* _Single points of failure._
* Poor fault tolerance (resiliency).
* Insufficient resources.
* Poor security.

And affecting long-term business growth...

* Complexity makes evolution difficult.

h1. Single Point of Failure (SPOF)

_A point in a system where a failure would cause the whole system to fail._

Examples:
* CPU, RAM, disk, network interface, power...
* Logical "tiers" and components:
** Database
** Middle-tier server
** Other network service

h1. Poor Fault Tolerance (Resiliency) 

*Fault Tolerance:* _How well a system continues to provide service if part of it fails or bad data is received._

Systems can be vulnerable even if they don't have any single points of failure. 

Examples:

* A database or other service query returns "invalid" data.
* A Browser with JavaScript disabled tries to display a page that relies on JavaScript.
* Bad user input!
* Software bugs!

h1. Poor Fault Tolerance (Resiliency) cont. 

The system should _degrade gracefully_.

Note that performance might suffer, even if a fault is tolerated.

You'll hear _stability_ and _reliability_ also used for essentially the same concept.

h1. Insufficient Resources

_The system can't meet the demands for service._

*Symptom:* The system can't service all clients within an acceptable window of opportunity, usually because there are too many concurrent clients!

Examples _Bottlenecks_:
<div class='small'>
* Disk or memory space.
* Network bandwidth.
* Database transaction throughput.
* Computation speed.
* Physical space! (i.e., enough room to add more "stuff")
</div>

h1. Poor Security

_Systems connected to the Internet are more vulnerable than closed systems._

... but security breaches "from the inside" are also significant.

* Malicious users.
* Unwitting users.

h1. Complexity Makes Evolution Difficult

_A poorly structured system is difficult to extend to support new functionality._

Example Sources of Complexity:
* Source code.
* System _topology_.
** How logical work is divided amongst _components_, _services_, and _network nodes_.
* Excessive coupling between _service points_.
* Too _little_ distribution (i.e., single _service points_ with too many responsibilities tend to be overly complex).

h1. Large-scale System Design Concerns

* Scalability
* Availability
* Security:
** Authentication
** Authorization

h1. Scalability

_Can the system's capacity be increased at reasonable cost?_ 

Affects the ability to provide adequate resources to meet service expectations.

Consider _latency_ vs. _throughput_, using a pipe analogy:
* *Latency:* The time it takes for any one computation. Imagine the pipe is _long_ or _short_.
* *Throughput:* The amount of computation per unit time. Imagine the pipe is _fat_ or _thin_.

h1. Scalability (cont.)

Two ways to Scale:

* Scale _vertically_.
* Scale _horizontally_.

h1. Scaling Vertically

Adding more hardware to the _same_ machine.

* *Easy:* Requires no software changes, no increase in "rack" space, etc.
* *Expensive:* Premium price for the top-of-the-line hardware.
* *SPOF:* The server remains a single point of failure

h1. Scaling Horizontally

Adding more hardware _in parallel_.

* *Hard:* May require significant software changes.
* *Cheap:* Can use lower-cost hardware (but more power and space?).
* *SPOF Elimination:* _Redundancy_ eliminates SPOF (in principle...)

h1. Redundancy

* N > 1 identical _components_, _services_, or _network nodes_.
* Enables _failover_ when one element fails.


h1. "Let It Crash"

A core philosophy in Akka, Erlang, and many other Actor-based systems. Rather than create big processes that require complex error recovery (which is very hard to do well...), use lots of small "workers" that are expendable. If any one of them gets into trouble, let it crash and restart it.

Requires careful partitioning and supervision of work, including tolerance for small failures.  

h1. Distribution of Computation and Data

* HW and SW can be tuned for specific services.
** E.g., The database servers have different performance requirements than the servers that deliver static web content (images, videos, JavaScript, ...).
* Data can be divided.
** Disk _striping_ (e.g., RAID-5 disk arrays). Gives both scalability (N disk drives) plus failure resiliency (data is effectively duplicated across the array). Disk array configurations can also parallelize writes, increasing _throughput_.
** Database _sharding_ (e.g., all East-coast customers in the New York data center). Data is separated, not duplicated (use _replication_ for that...)

h1. Caching

_Remembering the result of an expensive operation for subsequent use._

* *Data Caching:* 
** _In-memory_ cache speeds up _disk_ I/O. 
** _CPU_ cache speeds up _memory_ I/O.

* *Memoization:*
** Remembering the result of a previous _calculation_.

h1. Availability

_Can the system provide service when clients expect it?_

Subject to the _service level agreements_ (SLAs) with clients about when service is required: 24x7? 8-5 week days? holidays? (Performance is usually part of SLAs, too.)

Requires _fault tolerance_.

h1. Availability Numbers

table(tiny).
|_.Availability&#160;% |_.Downtime/Year  |_.Downtime/Month |_.Downtime/Week |
| 90% ("one nine")  | 36.5 days  | 72 hours  | 16.8 hours  |
| 95%  | 18.25 days  | 36 hours  | 8.4 hours  |
| 98%  | 7.30 days  | 14.4 hours  | 3.36 hours  |
| 99% ("two nines")  | 3.65 days  | 7.20 hours  | 1.68 hours  |
| 99.5%  | 1.83 days  | 3.60 hours  | 50.4 minutes  |
| 99.8%  | 17.52 hours  | 86.23 minutes  | 20.16 minutes  |
| 99.9% ("three nines")  | 8.76 hours  | 43.2 minutes  | 10.1 minutes  |
| 99.95%  | 4.38 hours  | 21.56 minutes  | 5.04 minutes  |
| 99.99% ("four nines")  | 52.56 minutes  | 4.32 minutes  | 1.01 minutes  |
| 99.999% ("five nines")  | 5.26 minutes  | 25.9 seconds  | 6.05 seconds  |
| 99.9999% ("six nines")  | 31.5 seconds  | 2.59 seconds  | 0.605 seconds  |
| 99.999999% ("nine nines")  | 31.5 microseconds  | 2.59 microseconds  | 0.605 microseconds  |

(adapted from http://en.wikipedia.org/wiki/High_availability)

h1. Achieving High Availability

* *Redundancy:* Improves fault tolerance (e.g., SPOF elimination).
* *Resource Planning:* Estimating _and measuring_ resource utilization. Avoidance of high % utilization.
* *Other Fault Tolerance Design Concerns:*
** Clock Synchronization
** Consensus
** Self Stabilization

h1. Clock Synchronization

_When the *timing of events* matters across different systems, how do you ensure that those systems have synchronized clocks?_

Examples: 
* Message queues.
* Log monitoring.

h1. Consensus

_How do you decide which systems are correct if answers that should be the same are different?_

There are four, redundant flight-control computers on the space shuttle:
<br/>

bq. The four general-purpose computers operate essentially in lockstep, checking each other. If one computer fails, the three functioning computers "vote" it out of the system. This isolates it from vehicle control. If a second computer of the three remaining fails, the two functioning computers vote it out. In the rare case of two out of four computers simultaneously failing (a two-two split), one group is picked at random.
(from the "Space Shuttle Wikipedia page":http://en.wikipedia.org/wiki/Space_Shuttle)

h1. Self Stabilization

Trying to "engineer-in" correctness in a distributed system is essentially impossible. Instead, design the system so it _constantly drives itself towards stability_.

This may or may not involve actual detection of errors.

h1. Security

* *Authentication:* Reliably identifying a client (user or external system) attempting to use system resources.
* *Authorization:* Once a client is authenticated, controlling access to those resources for which the client is permitted to use.


h1. Topologies

* Client–server (2-tier) architecture
* 3-tier architecture
* N-tier architecture
* Tightly coupled (clustered)
* Peer-to-peer
* Space based

h1. Client–server (2-tier) Architecture

A client requests data or work from a server, or sends data updates to a server. The client then formats and presents the results to the user. Often, the server is a database and the client implements non-trivial "business domain logic" (business logic, for short).

Note that _tier_ often implies a vertical orientation; _client down to server and back up to client_.

Examples:
* Typical VisualBasic apps.
* Simpler, low-volume web applications.

h1. 3-tier Architecture

A presentation tier, a middle tier handling "business logic", and a database (persistence) tier. The middle tier implements most of the logic that would be in the client and some of the logic that would be in the "server" (database?) in a 2-tier system.

* The "standard" architecture of mid-sized, mid-volume Internet and IT systems.
* Having three tiers simplifies the responsibilities of each tier, compared to 2-tier systems.
** Putting logic in the UI is bad because it's hard to test and it doesn't cleanly separation "concerns".

h1. N-tier Architecture

Generalization of 3-tier architecture. There may be more than 3 vertical tiers, but often there are _horizontal_ (peer) services that separate responsibilities, usually only at the mid tier and persistence "levels" (but the UI should be _componentized_).

Typical of all high-volume, high-availability web sites and IT applications.

Examples:
* Cloud computing
* Email?

h1. Tightly Coupled (Clustered)

Usually refers to a cluster of servers or services that collaborate closely, perhaps to solve a problem with _divide and conquer_ after which the individual contributions are combined to produce the final result.

Tight coupling may imply that failure of one element causes the work of all elements to fail. An exception is typical _map-reduce_ systems (which we'll discuss in a subsequent lecture), like _Hadoop_, which handle single node failures.
 
Example:
* Multicore Processor
* Map-Reduce

h1. Master-Slave

An arrangement where one system dictates the work performed by subordinate systems.

Also used for arrangements of similar systems where one system is designated the primary source of data and/or computation (the "master"), but work may _fail over_ to a backup ("slave") system if the master fails. The slave(s) may have to do all the work the master does (e.g., store data records) in order to take over. _Primary-secondary_ is sometimes used to describe this arrangement.

The slave is now considered the master. After recovery, the old master may re-enter the system as a slave (or become the master again).

Example:
* Datebase master-slave server

h1. Peer-to-peer

In contrast to _master-slave_, an architecture where no one system has a special role relative to the others in a group that provides a service. No one system directs the work of others, either. Rather, all systems divide the responsibilities equally, as _peers_, including the management of work. 

Example:
* _Mesh_ network

h1. Space Based

An infrastructure that creates the illusion of one single address-space. Data (and sometimes code!) are transparently replicated among "physical" elements. Sometimes called _virtualization_ (but that has other meanings, too).

Examples:
* Terracotta - Virtualization of data across JVM instances.
* Tuple spaces - e.g., Linda

h1.  Examples of Distributed Systems and Applications

For more examples, see the list on the "Distributed Computing":http://en.wikipedia.org/wiki/Distributed_computing Wikipedia page.

h1. Scaling Systems, Part II 

We'll return to _Scaling Systems_ next month after the Midterm. For the next few weeks, we'll focus on SQL and _NoSQL_ databases.

h1. Reading Assignments: 

* "Scalability, Availability, Stability Patterns":http://www.slideshare.net/jboner/scalability-availability-stability-patterns. A (long) presentation that covers many essentials of building systems that satisfy requirements we've discussed. It also discusses _NoSQL_ databases that we'll explore in subsequent weeks. Some of the other concepts discussed (like immutability and referential transparency) will be discussed later in the course. Skim the sections on data flow concurrency and event processing approaches; we won't cover them in the course. I also don't expect you to remember the lists of products in each category!
* "NoSQL, Heroku, and You":http://blog.heroku.com/archives/2010/7/20/nosql/. One cloud service vendor's take on the importance of NoSQL databases.

h1. For Further Research

* Diomidis Spinellis and Georgios Gousios, "Beautiful Architecture", O'Reilly, 2009. ISBN: 0596517984. Good essays on architecture.
* "Consensus":http://en.wikipedia.org/wiki/Consensus_(computer_science).
* "Self Stabilization":http://en.wikipedia.org/wiki/Self-stabilisation.
* "Byzantine fault tolerance":http://en.wikipedia.org/wiki/Byzantine_fault_tolerance.
* Martin Fowler, et al., "Patterns of Enterprise Application Architecture", Addison-Wesley, 2003. ISBN: 0321127420. Conventional, object-oriented approach. Does not cover the latest thinking on highly-distributed architectures.

h1. Add New Functionality (step 1)

This week, we'll finally add the NYSE data to the project.

Make sure you have pulled the latest changes from the project on the @COMP-388-488@ branch.

Open the @README.md@ file (it's nicely formatted on GitHub) and follow the directions in the newly-revised section called *Import the Data*. It explains the new scripts in the @bin@ directory (which we'll also discuss in class) that you use to import the data.

After importing the data, read the next section on using the web app, which explains a few things you'll need to know for the current state of the project.

h1. Add New Functionality (step 2)

Note that the instructions mention a @datatmp@ directory that holds temporary JSON files that were generated from the original YAML files and imported into MongoDB. These JSON files contain one-line JSON expressions as required by @mongoimport@. Once the data import is done and the @mongo@ console commands suggested in the README work correctly, you can delete @datatmp@.

h1. Add New Functionality (step 3)

For our next exercise (#2), we'll answer the question, "What stocks are available?". We'll add a feature that returns the available stock symbols.

There is a new branch on the GitHub project, based on @COMP-388-488@, called @exercise2@. run these commands, where the @#...@ are comments:

:inlinecode lang=bash, class=code-tiny
git fetch                        # grab the latest off GitHub, but don't merge changes
git branch -a                    # Show all the local and remote branches
git log origin/COMP-388-488 ^COMP-388-488  # show me what's changed!
git checkout COMP-388-488        # move to this branch (if not already there)
git merge origin/COMP-388-488    # merge to my local branch
git branch --track exercise2 origin/exercise2   # create local copy of "exercise2"
git checkout exercise2           # change to the "exercise2" branch
# start working...
:endinlinecode 

Note that @git pull@ is the combination of @git fetch@ and @git merge@.

h1. Add New Functionality (step 4)

Run jetty in @sbt@ (i.e., @jetty-run@) and go to the web page. The @bogus@ button is gone and there is now a @List Stocks@ button. If you click it, you'll get the same sort of error message that @bogus@ triggered. Your mission is to make this work.

The UI code is already prepared. You just need to modify the server code, starting with the REST interface.

h1. Add New Functionality (step 5)

* Edit @RestfulDataPublisher.scala@. We need to do a little _refactoring_ first.
* Change @getStatsFromInstrumentAnalysisServerSupervisors@ to take this argument: @message: InstrumentCalculationMessages@.
* Change @supervisor !! CalculateStatistics(allCriteria)@ to @supervisor !! message@.
* Now this method is more general to meet our needs.
 
h1. Add New Functionality (step 6)

* Also in @RestfulDataPublisher.scala@, in method @getAllDataFor@, change:

:inlinecode lang=scala, class=code-tiny
val results = getStatsFromInstrumentAnalysisServerSupervisors(allCriteria)
:endinlinecode 

to

:inlinecode lang=scala, class=code-tiny
val results = getStatsFromInstrumentAnalysisServerSupervisors(CalculateStatistics(allCriteria))
:endinlinecode 

* Run the tests; they should all pass!
* In what follows, use the sbt @~test@ target to keep your tests passing as much as possible. There will be some intermediate steps when they won't pass.

h1. Add New Functionality (step 7)

* Edit @InstrumentAnalysisServer.scala@.
* Add this new object definition after the definition of @CalculateStatistics@:

:inlinecode lang=scala, class=code-small
case class GetInstrumentList(range: scala.collection.immutable.NumericRange[Char]) extends InstrumentCalculationMessages
:endinlinecode 

* (You can import @NumericRange@ if you don't want to put this fully-qualified name in the declaration.)

h1. Add New Functionality (step 8)

* Back in @RestfulDataPublisher.scala@, edit the @restRequest@ method.
* Add a case statement for the string "list_stocks". (Follow the example for "stats").
* Have the case handler call a method similar to @getAllDataFor@. Call it @getAllInstruments@. Pass one argument; the @instruments@ variable, of type @String@.

h1. Add New Functionality (step 9)

* If you copied and pasted @getAllDataFor@ ;), change the first several lines in the @try@ block to the following lines:

:inlinecode lang=scala, class=code-tiny
// Hack! Just grab the first and last letters in the "instruments" string
// and use them as the range, but handle the case of 0 or 1 characters.
val symbolRange = instruments.trim match {
  case "" => `A` to `Z`  // default
  case s  => s.length match {
    case 1 => `A` to s.charAt(0).toUpper  // 1 character; use as end
    case n => s.charAt(0).toUpper to s.charAt(n-1).toUpper
  }
}
val results = getStatsFromInstrumentAnalysisServerSupervisors(GetInstrumentList(symbolRange))
val result = compact(render(JSONMap.toJValue(Map("instrument-list" -> results))))
:endinlinecode 
 
* *Warning:* those backticks ` should be forward ticks ' (formatting problem!).

h1. Add New Functionality (step 10)

* Edit @InstrumentAnalysisServerSupervisors.scala@.
* In the @defaultHandler@, add this case clause.

:inlinecode lang=scala, class=code-tiny
case GetInstrumentList(range) => self.reply(getInstrumentList(range))
:endinlinecode 

h1. Add New Functionality (step 11)

* Create the @getInstrumentList@ method, which takes one argument: @range: scala.collection.immutable.NumericRange[Char]@. It should look similar to the @calculate@ method. The main difference is the @for@ "comprehension" (loop):

:inlinecode lang=scala, class=code-small
val futures = for {
  letter <- range  // Recall that range is something like 'A' to 'D'
  oneLetterRange = letter to letter
  instrument = Instrument(letter.toString)
  calculator <- getOrMakeInstrumentAnalysisServerFor(instrument, Price(Dollars))
} yield (calculator !!! GetInstrumentList(oneLetterRange))
:endinlinecode 

h1. Add New Functionality (step 12)

* Edit @InstrumentAnalysisServer.scala@.
* Add the same case clause to @defaultHandler@ that you added in @InstrumentAnalysisServerSupervisors.scala@.
* However, change the call to a @getInstrumentList@ method to be a call on the @helper@ object in the file.
* Create this method in the helper class. It must take the same one-argument that @InstrumentAnalysisServerSupervisors.getInstrumentList@ took.
* Copy the body of @fetchPrices@ to @getInstrumentList@. 
* Make this new method public, not @protected@! (In Scala, the default is public)

h1. Add New Functionality (step 13)

* In @getInstrumentList@, change the argument to the @Get@ object that's sent to @dataStorageServer@ to @Pair("instrument_list", range.toList.head.toString)@. (We'll just assume here that there is only one character in the range; we're using one instance of this class per letter.)
* Change the error message for the @None@ case to something appropriate.
* Change the @Some@ case to simply return the @result@.

h1. Add New Functionality (step 14)

* Implement @formatInstrumentListResults(result, range)@. Follow the example of @formatPriceResults@. 
* Instead of the @criteria@ entry in the Map, return "range" as the key and @range.toString@ as the value.
* Keep the @results@ part as is.

h1. Add New Functionality (step 15)

* Edit @DataStorageServer@.
* Rename @getData@ to @getDataForRange@ 
* _Copy_ the function signature for @getDataForRange@ and create a new @getData@ with it. Use the following body:

:inlinecode lang=scala, class=code-small
(criteria \ "instrument_list") match {
  case JField(key, value) => getInstrumentList(value)
  case _ => getDataForRange(criteria)
} 
:endinlinecode 

h1. Add New Functionality (step 16)

* Create method @getInstrumentList@. 
* Here we will use a hack to reduce the amount of code we would have to write for the general case. We'll only worry about getting the list from MongoDB, not all data stores. (How could you eliminate this hack with a more general solution??)

:inlinecode lang=scala, class=code-tiny
protected[persistence] def getInstrumentList(prefix: String): JValue = dataStore match {
  case mongo: MongoDBDataStore => 
    val data = for {
      json <- mongo.getInstrumentList(prefix)
    } yield json
    toJSON(data toList)
  case _ => throw new RuntimeException("Can't get the instrument list from datastore "+dataStore)
}
:endinlinecode 

h1. Add New Functionality (step 17)

* Add these imports to @MongoDBDataStore@.

:inlinecode lang=scala, class=code-small
import net.liftweb.json.JsonAST._
import net.liftweb.json.JsonDSL._
import org.joda.time.format._
:endinlinecode 

h1. Add New Functionality (step 18)

* Create method @getInstrumentList@ in @MongoDBDataStore@. Here it is:

:inlinecode lang=scala, class=code-small
// Hack!
def getInstrumentList(prefix: String): Iterable[JSONRecord] = try {
  // TODO: We hard-code the name of the thing we want, the "stock_symbol". Should be abstracted...
  val list = collection.distinct("stock_symbol")
  val buff = new scala.collection.mutable.ArrayBuffer[String]()
  while (list.hasNext) {
    buff += list.next
  }
  // Must put in a timestamp to make JSONRecord happy:
  val format = DateTimeFormat.forPattern("yyyy-MM-dd")
  List(JSONRecord(("date" -> format.print(new DateTime)) ~ ("letter" -> prefix) ~ ("symbols" -> buff.toList)))
} catch {
  case th => 
    log.error("MongoDB Exception: ", th)
    throw th
}
:endinlinecode 

h1. Add New Functionality (step 19)

Try it out! use @jetty-run@ in @sbt@, enter two letters for the @Symbols@ in the UI and click @List Stocks@. You should get a table with one row for all the symbols that start with the letter @A@, one row for @B@, etc.

*Warning:* If might run out of memory if you ask for @A-Z@ symbols! Try small ranges first.

Make sure the price queries still work.

h1. Add New Functionality (step 20)

We did all these changes without adding tests for them. That's not really the best way to work (as we'll see later in the course). Go back through the changes and create tests for as many of them as you can. Where we copied existing code, find tests for the original code and see if you can copy and adapt those tests.

h1. Add New Functionality (final)

Please commit your changes to GitHub by the start of class next week.
