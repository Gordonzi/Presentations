h1. Lecture 6: Scaling Data, Part III

We end our tour of new database technologies with a discussion of column-oriented databases and graph databases.

h1. Cassandra: A Column-Oriented Database

Design goals:
* Reliability through distributed processing, no single point of failure.
* High write throughput.
* Trade RDBMS features for a simple data model with easy, dynamic control over data layout and format.

h1. Cassandra: Developed at Facebook

Originally designed for Facebook's Inbox Search feature.
* Very high write throughput required.
* Replication to geographically-distributed data centers.
** Minimizes search latencies.

h1. Cassandra: Inspirations

* Inspired by Amazon's Dynamo and Google's BigTable.
** But provides better write performance.
* Open-source Apache Project.
** But Facebook has maintained its own fork.

h1. Cassandra: Columns

_Column Oriented_ means that columns are easy to query for, as well as to add and remove in the "schema". This supports evolution of features.

However, operations are keyed by rows and each row operation is atomic, independent of the number of columns involved.

h1. Cassandra: Columns

* *Column Families:* a group of columns.
** *Super Column Family*: a column family within a column family.

h1. API

Just 3 methods:
* insert(table, key, rowMutation)
* get(table, key, columnName)
* delete(table, key, columnName)

_columnName_ can refer to a single column, or a column family.

h1. Distributed System Techniques

Cassandra uses many distributed system techniques for scaling and availability. We'll discuss these:
* Partitioning
* Replication
* Membership
* Failure Handling
* Scaling

h1. Partitioning

Dynamically partition the data over the cluster.
* To hard to do this manually.
* Keys are hashed, using consistent hashing.
* The range of possible hash values is treated as a "ring" that wraps.
* Each node is assigned a random position on the ring.
 
When a row's key is hashed, the node with the closest key _greater than_ the row's key is assigned the responsibility for coordinating reads and writes. 

h1. Reads and Writes

The responsible node decides what replicas should also handle the request.
* Reads can be configured to use the value of the nearest replica or a _quorum_ of replica responses.
* Writes are routed to all the designated replicas and the write is only considered complete when a _quorum_ of replicas respond that the write was successful.

h1. Failures and Balancing

If a node fails or a new one is added, only its immediate neighbor nodes in the ring are affected.

The initial distribution of nodes doesn't account for the actual distribution of keys in the data nor the performance of particular nodes. Hence, it can be imbalanced. Cassandra will move nodes on the ring to adjust performance.

h1. Replication

Each Cassandra "instance" is configured with a _replication factor_ of N, the number of nodes to which each data item is replicated. 

When a node is responsible for a row, it stores the row locally and replicates it to N-1 other nodes.

Cassandra can be configured to ensure that each row is replicated across multiple data centers, so every row remains available even in the event of a total failure of a data center.

h1. Membership

Membership and health of the nodes is managed through a _Gossip_ protocol called _Scuttlebutt_. The _Phi Accrual Failure Detector_ module is used by each node to publish its "suspicions" about the health of every other node, rather than publishing an "up" or "down" report that may not be accurate. The suspicion level rises, as concerns about a node rise. At the same time, the probability of error decreases.

This turns out to be a much faster way to detect node failure than other detection schemes.

h1. Bootstrapping 

When a node starts for the first time, it chooses a random position on the ring, then "Gossips" its position to the cluster. This is how all the other nodes know the members of the cluster, for replication, etc.

h1. Scaling

A new node can be assigned a key that allows it to offload an overloaded node. The overloaded node then uses kernel-to-kernel copying techniques to move the data it is no longer responsible for to the new node.

h1. Local Persistence: Writes

For local storage, Cassandra relies on the local file system. (By contrast, Google's BigTable is designed for GFS, a distributed file system.)

Writes are appended to a commit log. Only _after_ this succeeds is the same change written to an in-memory data structure.

Facebook uses a dedicated disk on each system for the commit log, to minimize seeks and thereby optimize writes.

When the in-memory storage reaches a certain size threshold, it is written to another disk and the rows are indexed for fast retrieval.

h1. Local Persistence: Writes

Over time, many such files written from the in-memory data structure will exist. A background process merges them into one file. Also, old commit logs are eventually deleted, once the in-memory structures they represent are persisted. 

Hence, durability is ensured through a combination of the commit log and the row files, replicated across the cluster.

Since no files are ever modified for updates, no locks are required. Cassandra also groups writes to be synchronous, maximizing disk write speeds.

h1. Local Persistence: Reads

First, the in-memory data structure is searched for the row. If not found, the _newest_ to _oldest_ files are searched, in order. 

To avoid searching a file that is unlikely to contain the row, a _bloom filter_ that summarizes the keys in the file is also stored in the file and and memory. It is examined to determine if the file has the row.

Column locations are also indexed, so getting to the correct disk block is optimized.

h1. Cassandra and Column Databases: Conclusion

* Great for schema that need to evolve by adding columns.
* Great for fast queries by column(s), without reading entire rows.
* Optimized for write performance, while maintaining fast reads.
* Trade full RDBMS features for flexibility and performance.

h1. Graph Databases

Graph databases model relationships as first class features. 

Contrast with relational databases, where relationships are managed through joins over keys from one set of records to another set of records, either in the same table or different tables. For M:N relationships, you have to manually create a separate _join table_. For data best modeled as a _network_ of _nodes_, this isn't very convenient, nor does it perform well, in most cases.

h1. Graph Examples

* The Internet!
* Physical maps: streets, public transit, geography, etc.
* Complex business rules.
* Manifests for shipping.
* Parts lists and their relationships for assembly lines and finished products (e.g., airplanes!).

h1. Graph Databases and OO Middleware

An _object_ is actually a graph of smaller objects:
* Smaller graphs (collections).
* "Leaf" nodes, like integers, floats, etc. 

(It's sometimes useful to treat other small objects, like strings as leaf nodes, too.)

Hence, for applications with complex object graphs, a graph database might be a better fit than a relational database.

h1. Elements of a Graph Database

At its core a graph database supports the following concepts from standard graph theory.

* Nodes
* Arcs
* Properties

h1. Nodes

A node is analogous to a row in a relational database. In a graph for a particular application, it is an "entity" whose relationships to other entities are of primary interest. 

Examples:
* Twitter, Facebook, LinkedIn, etc. users.
* Airports served by an airline.
* Addresses to which mail and packages are delivered.

h1. Arcs

The connections (relationships) between nodes. Can be unidirectional or bidirectional. 

Examples:
* Your followers and who you follow on Twitter.
* Your friends and their friends, ad infinitum, on Facebook, LinkedIn, etc.
* Flight "legs" between two airports.
* Delivery route between two addresses for mail and/or package deliveries.

Which of these are always bidirectional? Sometimes?

h1. Arcs (cont.)

Traversals (paths) are formed from multiple, contiguous arcs.
* They may be circular.
** The may go from one node to another and back again.
* There will be dead ends.
* There will be clumps with relatively few arcs between the clumps.

For a real-world example, see this video of a "LinkedIn employee's own social graph":http://www.youtube.com/watch?v=se2u5RyGaNE.

h1. Properties

Key-value pairs associated with nodes and arcs. Most of the interesting data is in these properties!

Example Properties for Nodes:
* Where you live, your profession, interests, age, marital status, etc., etc.
* Airport capacity, number of gates, hours of operation, etc.
* Business or personal address? Apartment building or free standing house? Hostile dogs?

h1. Properties (cont.)

Example Properties for Arcs:
* Is this a professional or personal relationship.
* How often is this relationship used?
** Twitter direct messages.
** Emails sent.
* Daily flight schedule (including number of flights, passengers carried per flight, etc.).
* Distance between two addresses.
* Hazards or barriers between two addresses.

h1. Graph Databases vs. Others

Recall that most of the database _kinds_ we've discussed can scale through _sharding_. That *assumes* there are relatively few relationships between the shards. Graph databases _emphasize_ connections, so sharding requires careful understanding of the actual clumps in the data. 

Where _joins_ can be expensive in an RDBMS, graphs optimize traversals.

h1. Neo4J

"Neo4J":http://neo4j.org and its Wikipedia "page":http://en.wikipedia.org/wiki/Neo4j.

Neo4j is a graph database. It is an embedded, disk-based, fully-transactional Java persistence engine that stores data structured in graphs rather than in tables.

h1. Neo4J: Features

* Object-oriented Java API.
* massive scalability; handles graphs of several billion nodes/relationships/properties on a single machine and can be sharded to scale out across multiple machines.
* Support for common RDBMS database features: 
** ACID transactions.
** Durable persistence.
** Concurrency control.

h1. FlockDB

"Wikipedia":http://en.wikipedia.org/wiki/FlockDB and on "GitHub":http://github.com/twitter/flockdb.

Goals:

* High throughput for add/update/remove operations.
* Support complex arithmetic queries.
* Paging through query result sets with millions of entries.
* Archiving of edges with later retrieval.
* Horizontal scaling, including replication.
* Online data migration

h1. FlockDB

Non-goals include:

* Multi-hop queries (or graph-walking queries)
* Automatic shard migrations

FlockDB tries to solve fewer problems, such as Neo4J, so it is much simpler than other graph databases.

h1. Developed at Twitter

Twitter uses FlockDB to store social graphs (who follows whom, who blocks whom) and secondary indices. As of April 2010, the Twitter FlockDB cluster stores 13+ billion edges and sustains peak traffic of 20k writes/second and 100k reads/second.

Written in Scala.

h1. Gephi: An Open-Source Tool for Graph Analysis.

"gephi.org":http://gephi.org.

An open-source tool for visualizing and analyzzing large networks graphs. 
* Uses a 3D render engine to display graphs in real-time.
* Supports data exploration, analysis, filtering, manipulation, and graphing of graph data.

h1. Gephi: Gotchas

It's a Java app that uses a lot of RAM. 

The included example graph, of the relationships between characters in the Phantom of the Opera, runs fine with the default configuration.

For larger graphs, follow the instructions for increasing the heap size. Only run it on a reasonably powerful machine.

h1. Graph Databases: Conclusion

"Survey of Graph Database Models":http://portal.acm.org/citation.cfm?id=1322433 provides a detailed survey (requires ACM online access).

"The Graph Traversal Pattern":http://nosqlsummer.org/paper/graph-traversal-pattern is a technical paper on graph databases.

h1. Reading Assignment: Cassandra

* "Apache Cassandra":http://en.wikipedia.org/wiki/Apache_Cassandra (Wikipedia).
* Avinash Lakshman and Prashant Malik, "Cassandra - A Decentralized, Structured Storage System":http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf.

h1. Reading Assignment: Graph Databases

Rather than a reading on graph databases, read this Wikipedia page on graphs in computer science, which describes the fundamental terminology used in graph databases, too.

* <a href="http://en.wikipedia.org/wiki/Graph_(data_structure)">Graph data structure</a> (Wikipedia).

h1. Reading Assignment: MapReduce

In preparation for our next class, when we'll finally discuss MapReduce, read the classic paper that introduced the first MapReduce framework, used at Google.

* "MapReduce":http://en.wikipedia.org/wiki/MapReduce (Wikipedia).

h1. Exercise: Filter Stock Queries

Since we started, the project has had one major flaw; if you query for closing prices for a list of stocks, you get the results for _all_ stocks in the same tables. We did one fix in an earlier exercise, where we filtered the results after the database query was made. Now let's do it in a more appropriate way, as part of the query itself.

h1. Exercise: Filter Stock Queries (cont.)

Several pieces are already in place. @RestfulDataPublisher@ already sends the "instrument" information to the @InstrumentAnalysisServerSupervisor@ in a @CriteriaMap@ object. (@CriteriaMap@ wraps a normal @Map@ with some convenience methods.)

@InstrumentAnalysisServerSupervisor@ sends the instrument information to the @InstrumentAnalysisServers@, in a modified @CriteriaMap@. However, @InstrumentAnalysisServer@ does not pass this information any further down the stack.

h1. Exercise: Filter Stock Queries (cont.)

Look at @InstrumentAnalysisServerHelper.fetchPrices@ (in @InstrumentAnalysisServer.scala@). Note the @Map@ it passes to @DataStoreServer@. It passes a @Map@ object, rather than a @CriteriaMap@, to provide a clean separation between the "domain-specific" @CriteriaMap@ and @InstrumentAnalysisServer@ higher in the "stack", and the more generic @DataStoreServer@ and @DataStore@ types lower down:

:inlinecode lang=scala, class=code-tiny
dataStorageServer !! Get(Map("start" -> start, "end" -> end))
:endinlinecode 

(Note, I recently refactored the project to use a @Map@, rather than a Lift JSON @JValue@, due to problems with the latter...)

h1. Exercise: Filter Stock Queries (cont.)

@("start" -> start)@ is converted to a @Pair[String, org.joda.time.DateTime]@ before they are passed to the @Map@ "factory" method. For example, if the @start@ value is "2010-10-01" and @end@ is "2010-10-05", the map would be.

:inlinecode lang=javascript, class=code-small
Map("start" -> "2010-10-01", "end" -> "2010-10-05")
:endinlinecode 

h1. Exercise: Filter Stock Queries (cont.)

Step 1: Pull the "exercise4_start" branch from GitHub. 

I pushed a new branch to GitHub called "exercise4_start". You'll want to start from there!

* Follow the instructions in early notes for pulling new branches from my GitHub repo. Pull my "exercise4_start".
* Create a local work branch called "exercise4".
* Merge my "exercise4_start" to your "exercise4".

h1. Exercise: Filter Stock Queries (cont.)

Step 2: Pull the "exercise4_start" branch from GitHub. Start @sbt@ 

* Start @sbt@.
* Run the @~test@ action to continuously check for broken code.

h1. Exercise: Filter Stock Queries (cont.)

Step 3: Add the instruments

* Add another @Pair@ to the @Map@ inside the @Get@ message object sent to the @dataStoreServer@. Use @stock_symbol@ as the key and a new list, named @symbols@ as the value. Note that the @instruments@ parameter to the method is of type @List[Instrument]@. All we want is a @List[String]@, where each @String@ is the @symbol@ field in the corresponding @Instrument@.

* Create a local @val@ named @symbols@. Initialize it with the list of symbols (@Strings@) created by calling the @map@ method on @instruments@ and passing a block that extracts the @symbol@ field from each instrument. (Hint: there is already a method to do this, in @object Instrument@.)
* In the @Get(Map(...))@ sent to the @dataStorageServer@, add the key-value pair with "stock_symbol" as the key and the new @symbols@ list as the value.

h1. Exercise: Filter Stock Queries (cont.)

Note that @InstrumentAnalysisServer@ has many details about how domain concepts, like "instruments" are mapped to the actual storage details, like "stock_symbol", so that @DataStorageServer@ and the @DataStore@ types can remain relatively agnostic about domain concepts. This makes them more reusable for other applications.

h1. Exercise: Filter Stock Queries (cont.)

Step 4: Extend @DataStorageServer@ to use other query parameters.

In @DataStorageServer.getDataForRange@, the @criteria@ object is passed in as an argument.

* Add this @criteria@ object as the _third_ argument in the call to @DataStore.range@. (At this point, the compilation will fail in @sbt@.)

h1. Exercise: Filter Stock Queries (cont.)

Step 5: Extend the @DataStore@ types to use this query object.

* Modify the declaration of @DataStore.range@ to accept this argument *before* the @maxNum@ argument.
** Give it the name @otherCriteria@.
** Give it a default value of @Map.empty@ (one way of specifying an empty map). In other words, the @otherCriteria@ argument will have this signature: @otherCriteria: Map[String,Any] = Map.empty@. (Note that the type parameters aren't required on the right-hand side of the equals sign. Do you understand why?)

* Add the same argument to the definitions of @InMemoryDataStore.range@ and @MongoDBDataStore.range@.

h1. Exercise: Filter Stock Queries (cont.)

Step 6: Fix the tests.

At this point, there should be a few test failures, all caused by changes that need to be made in @DataStoreTestBase@. Specifically, we are still calling the @range@ method on a @DataStore@ object without the new, third argument. Note that this is only a problem for the cases where we explicitly pass the @maxNum@ argument. (Do you understand why that's true?)

* Look at the test error messages and find the lines in @DataStoreTestBase@ where the changes are required.
* Insert @Map.empty@ as the _third_ argument to the @range@ call, before the current third argument, the @maxNum@ value.

Now the tests should pass!

h1. Exercise: Filter Stock Queries (cont.)

Step 7: Continue implementing the new query support.

Go back to @MongoDBDataStore@ and look at the @range@ method. Now we'll use the new @otherCriteria@ object. I recently refactored the code to use a MongoDB @QueryBuilder@. We need to append additional information to the query. To keep the exercise simpler, we'll hard code the fact that @otherCriteria@ may now have a key-value pair @("stock_symbol", List[String])@. (I say "may have", because clients won't always pass a @otherCriteria@ map with this key-value pair, like some of the tests in @DataStoreTestBase@!)

* Using the Scala @Map@ API, add a conditional that tests for the presence of this key in @otherCriteria@.
* ...

h1. Exercise: Filter Stock Queries (cont.)

Step 7 (cont.): Continue implementing the new query support.

* If the key is found, add the key-value pair to the query:
** Use the @QueryBuilder@ code already in the method as an example. 
** Find the JavaDocs for @QueryBuilder@ and @BasicDBList@ "here":http://api.mongodb.org/java/2.2/index.html. 
** Write code that creates a @BasicDBList@ and adds each element from the @symbols:List[String]@ to it. (That is, we have to convert the Scala @List@ to something MongoDB understands: @BasicDBList@.)
** Use the @and(key)@ method and the @in(myBasicDBList)@ methods on the @QueryBuilder@ object (i.e., add these changes *before* the line that calls @qb.get@!).

h1. Exercise: Filter Stock Queries (cont.)

Step 8: Try it out!!

Run the application and open the web page in a browser. Enter a few stock symbols and date ranges. You should now see results only for the stocks you specified, not for all stocks in the same collection!

Note that we have left out some work; besides hard-coding a particular enhancement, rather than handling the general case, we haven't supported this change in @InMemoryDataStore@ and we haven't updated our tests! We'll fix this later.

Please push your work to GitHub by the start of class on October 19th.

h1. Reminder

*No class* next week (10/12).
*Midterm*, the first 1/2 of our session on (10/19)
